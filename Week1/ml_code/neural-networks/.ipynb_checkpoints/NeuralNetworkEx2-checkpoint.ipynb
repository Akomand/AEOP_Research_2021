{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/ex2data1.txt')\n",
    "data = np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[['Exam1', 'Exam2']].values.reshape(-1,2)\n",
    "y = dataset[['Admitted']].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = trainData(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(y_train))\n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=1)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(2, 16) \n",
    "        self.layer_2 = nn.Linear(16, 16)\n",
    "        self.layer_out = nn.Linear(16, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(p=0.1)\n",
    "#         self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "#         self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "#         x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "#         x = self.batchnorm2(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = binaryClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "# batch_size = 64\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: | Loss: 0.89223 | Acc: 57.143\n",
      "Epoch 001: | Loss: 0.76468 | Acc: 55.714\n",
      "Epoch 002: | Loss: 0.73364 | Acc: 58.571\n",
      "Epoch 003: | Loss: 0.68352 | Acc: 55.714\n",
      "Epoch 004: | Loss: 0.66576 | Acc: 60.000\n",
      "Epoch 005: | Loss: 0.65619 | Acc: 62.857\n",
      "Epoch 006: | Loss: 0.63771 | Acc: 61.429\n",
      "Epoch 007: | Loss: 0.64351 | Acc: 61.429\n",
      "Epoch 008: | Loss: 0.62710 | Acc: 60.000\n",
      "Epoch 009: | Loss: 0.63198 | Acc: 62.857\n",
      "Epoch 010: | Loss: 0.62012 | Acc: 64.286\n",
      "Epoch 011: | Loss: 0.62580 | Acc: 62.857\n",
      "Epoch 012: | Loss: 0.61119 | Acc: 62.857\n",
      "Epoch 013: | Loss: 0.61716 | Acc: 67.143\n",
      "Epoch 014: | Loss: 0.60926 | Acc: 61.429\n",
      "Epoch 015: | Loss: 0.60081 | Acc: 67.143\n",
      "Epoch 016: | Loss: 0.60234 | Acc: 64.286\n",
      "Epoch 017: | Loss: 0.61162 | Acc: 62.857\n",
      "Epoch 018: | Loss: 0.60561 | Acc: 62.857\n",
      "Epoch 019: | Loss: 0.60845 | Acc: 62.857\n",
      "Epoch 020: | Loss: 0.60210 | Acc: 62.857\n",
      "Epoch 021: | Loss: 0.60342 | Acc: 61.429\n",
      "Epoch 022: | Loss: 0.60137 | Acc: 61.429\n",
      "Epoch 023: | Loss: 0.59551 | Acc: 65.714\n",
      "Epoch 024: | Loss: 0.60174 | Acc: 62.857\n",
      "Epoch 025: | Loss: 0.59870 | Acc: 64.286\n",
      "Epoch 026: | Loss: 0.60010 | Acc: 62.857\n",
      "Epoch 027: | Loss: 0.59956 | Acc: 62.857\n",
      "Epoch 028: | Loss: 0.59427 | Acc: 65.714\n",
      "Epoch 029: | Loss: 0.58089 | Acc: 62.857\n",
      "Epoch 030: | Loss: 0.59457 | Acc: 62.857\n",
      "Epoch 031: | Loss: 0.59459 | Acc: 65.714\n",
      "Epoch 032: | Loss: 0.58991 | Acc: 67.143\n",
      "Epoch 033: | Loss: 0.58305 | Acc: 64.286\n",
      "Epoch 034: | Loss: 0.57286 | Acc: 70.000\n",
      "Epoch 035: | Loss: 0.59494 | Acc: 62.857\n",
      "Epoch 036: | Loss: 0.59137 | Acc: 65.714\n",
      "Epoch 037: | Loss: 0.58838 | Acc: 64.286\n",
      "Epoch 038: | Loss: 0.58716 | Acc: 67.143\n",
      "Epoch 039: | Loss: 0.57919 | Acc: 64.286\n",
      "Epoch 040: | Loss: 0.58317 | Acc: 64.286\n",
      "Epoch 041: | Loss: 0.58703 | Acc: 65.714\n",
      "Epoch 042: | Loss: 0.57829 | Acc: 64.286\n",
      "Epoch 043: | Loss: 0.58803 | Acc: 65.714\n",
      "Epoch 044: | Loss: 0.57948 | Acc: 65.714\n",
      "Epoch 045: | Loss: 0.58360 | Acc: 65.714\n",
      "Epoch 046: | Loss: 0.58446 | Acc: 65.714\n",
      "Epoch 047: | Loss: 0.58320 | Acc: 64.286\n",
      "Epoch 048: | Loss: 0.58535 | Acc: 65.714\n",
      "Epoch 049: | Loss: 0.57551 | Acc: 65.714\n",
      "Epoch 050: | Loss: 0.57629 | Acc: 68.571\n",
      "Epoch 051: | Loss: 0.58242 | Acc: 65.714\n",
      "Epoch 052: | Loss: 0.57356 | Acc: 67.143\n",
      "Epoch 053: | Loss: 0.57553 | Acc: 65.714\n",
      "Epoch 054: | Loss: 0.58252 | Acc: 64.286\n",
      "Epoch 055: | Loss: 0.57527 | Acc: 64.286\n",
      "Epoch 056: | Loss: 0.57972 | Acc: 62.857\n",
      "Epoch 057: | Loss: 0.56675 | Acc: 65.714\n",
      "Epoch 058: | Loss: 0.57014 | Acc: 67.143\n",
      "Epoch 059: | Loss: 0.55883 | Acc: 70.000\n",
      "Epoch 060: | Loss: 0.57269 | Acc: 65.714\n",
      "Epoch 061: | Loss: 0.57651 | Acc: 64.286\n",
      "Epoch 062: | Loss: 0.57764 | Acc: 64.286\n",
      "Epoch 063: | Loss: 0.57131 | Acc: 67.143\n",
      "Epoch 064: | Loss: 0.57358 | Acc: 67.143\n",
      "Epoch 065: | Loss: 0.57145 | Acc: 62.857\n",
      "Epoch 066: | Loss: 0.57094 | Acc: 62.857\n",
      "Epoch 067: | Loss: 0.56411 | Acc: 67.143\n",
      "Epoch 068: | Loss: 0.57447 | Acc: 67.143\n",
      "Epoch 069: | Loss: 0.56599 | Acc: 65.714\n",
      "Epoch 070: | Loss: 0.56268 | Acc: 67.143\n",
      "Epoch 071: | Loss: 0.56940 | Acc: 65.714\n",
      "Epoch 072: | Loss: 0.56135 | Acc: 64.286\n",
      "Epoch 073: | Loss: 0.55622 | Acc: 68.571\n",
      "Epoch 074: | Loss: 0.57723 | Acc: 65.714\n",
      "Epoch 075: | Loss: 0.56638 | Acc: 70.000\n",
      "Epoch 076: | Loss: 0.56563 | Acc: 67.143\n",
      "Epoch 077: | Loss: 0.56458 | Acc: 67.143\n",
      "Epoch 078: | Loss: 0.56202 | Acc: 67.143\n",
      "Epoch 079: | Loss: 0.56317 | Acc: 64.286\n",
      "Epoch 080: | Loss: 0.56350 | Acc: 70.000\n",
      "Epoch 081: | Loss: 0.55157 | Acc: 70.000\n",
      "Epoch 082: | Loss: 0.56749 | Acc: 64.286\n",
      "Epoch 083: | Loss: 0.56803 | Acc: 67.143\n",
      "Epoch 084: | Loss: 0.56465 | Acc: 67.143\n",
      "Epoch 085: | Loss: 0.55724 | Acc: 68.571\n",
      "Epoch 086: | Loss: 0.56413 | Acc: 65.714\n",
      "Epoch 087: | Loss: 0.55664 | Acc: 68.571\n",
      "Epoch 088: | Loss: 0.55764 | Acc: 65.714\n",
      "Epoch 089: | Loss: 0.55042 | Acc: 67.143\n",
      "Epoch 090: | Loss: 0.56715 | Acc: 65.714\n",
      "Epoch 091: | Loss: 0.54600 | Acc: 67.143\n",
      "Epoch 092: | Loss: 0.55548 | Acc: 62.857\n",
      "Epoch 093: | Loss: 0.56143 | Acc: 67.143\n",
      "Epoch 094: | Loss: 0.56310 | Acc: 67.143\n",
      "Epoch 095: | Loss: 0.55815 | Acc: 67.143\n",
      "Epoch 096: | Loss: 0.56169 | Acc: 68.571\n",
      "Epoch 097: | Loss: 0.55014 | Acc: 64.286\n",
      "Epoch 098: | Loss: 0.56054 | Acc: 68.571\n",
      "Epoch 099: | Loss: 0.55081 | Acc: 68.571\n",
      "Epoch 100: | Loss: 0.56058 | Acc: 62.857\n",
      "Epoch 101: | Loss: 0.55920 | Acc: 68.571\n",
      "Epoch 102: | Loss: 0.56021 | Acc: 65.714\n",
      "Epoch 103: | Loss: 0.55680 | Acc: 64.286\n",
      "Epoch 104: | Loss: 0.55502 | Acc: 62.857\n",
      "Epoch 105: | Loss: 0.55680 | Acc: 65.714\n",
      "Epoch 106: | Loss: 0.55729 | Acc: 67.143\n",
      "Epoch 107: | Loss: 0.55305 | Acc: 65.714\n",
      "Epoch 108: | Loss: 0.55128 | Acc: 62.857\n",
      "Epoch 109: | Loss: 0.55190 | Acc: 67.143\n",
      "Epoch 110: | Loss: 0.55214 | Acc: 65.714\n",
      "Epoch 111: | Loss: 0.55387 | Acc: 65.714\n",
      "Epoch 112: | Loss: 0.54992 | Acc: 64.286\n",
      "Epoch 113: | Loss: 0.55010 | Acc: 70.000\n",
      "Epoch 114: | Loss: 0.55171 | Acc: 67.143\n",
      "Epoch 115: | Loss: 0.54606 | Acc: 71.429\n",
      "Epoch 116: | Loss: 0.54571 | Acc: 62.857\n",
      "Epoch 117: | Loss: 0.54651 | Acc: 64.286\n",
      "Epoch 118: | Loss: 0.54290 | Acc: 65.714\n",
      "Epoch 119: | Loss: 0.55714 | Acc: 67.143\n",
      "Epoch 120: | Loss: 0.54610 | Acc: 70.000\n",
      "Epoch 121: | Loss: 0.53232 | Acc: 67.143\n",
      "Epoch 122: | Loss: 0.55199 | Acc: 71.429\n",
      "Epoch 123: | Loss: 0.54004 | Acc: 65.714\n",
      "Epoch 124: | Loss: 0.55350 | Acc: 71.429\n",
      "Epoch 125: | Loss: 0.53804 | Acc: 67.143\n",
      "Epoch 126: | Loss: 0.54838 | Acc: 67.143\n",
      "Epoch 127: | Loss: 0.54874 | Acc: 68.571\n",
      "Epoch 128: | Loss: 0.54077 | Acc: 65.714\n",
      "Epoch 129: | Loss: 0.54820 | Acc: 68.571\n",
      "Epoch 130: | Loss: 0.51994 | Acc: 75.714\n",
      "Epoch 131: | Loss: 0.54828 | Acc: 67.143\n",
      "Epoch 132: | Loss: 0.52876 | Acc: 68.571\n",
      "Epoch 133: | Loss: 0.54403 | Acc: 65.714\n",
      "Epoch 134: | Loss: 0.53562 | Acc: 71.429\n",
      "Epoch 135: | Loss: 0.54772 | Acc: 64.286\n",
      "Epoch 136: | Loss: 0.54711 | Acc: 70.000\n",
      "Epoch 137: | Loss: 0.53788 | Acc: 71.429\n",
      "Epoch 138: | Loss: 0.54424 | Acc: 68.571\n",
      "Epoch 139: | Loss: 0.52755 | Acc: 71.429\n",
      "Epoch 140: | Loss: 0.54163 | Acc: 65.714\n",
      "Epoch 141: | Loss: 0.52263 | Acc: 68.571\n",
      "Epoch 142: | Loss: 0.54514 | Acc: 70.000\n",
      "Epoch 143: | Loss: 0.52864 | Acc: 65.714\n",
      "Epoch 144: | Loss: 0.53963 | Acc: 71.429\n",
      "Epoch 145: | Loss: 0.54069 | Acc: 70.000\n",
      "Epoch 146: | Loss: 0.54309 | Acc: 67.143\n",
      "Epoch 147: | Loss: 0.53161 | Acc: 70.000\n",
      "Epoch 148: | Loss: 0.52862 | Acc: 68.571\n",
      "Epoch 149: | Loss: 0.53748 | Acc: 68.571\n",
      "Epoch 150: | Loss: 0.53854 | Acc: 70.000\n",
      "Epoch 151: | Loss: 0.53504 | Acc: 70.000\n",
      "Epoch 152: | Loss: 0.53751 | Acc: 67.143\n",
      "Epoch 153: | Loss: 0.54195 | Acc: 67.143\n",
      "Epoch 154: | Loss: 0.52598 | Acc: 70.000\n",
      "Epoch 155: | Loss: 0.54009 | Acc: 70.000\n",
      "Epoch 156: | Loss: 0.53152 | Acc: 71.429\n",
      "Epoch 157: | Loss: 0.54056 | Acc: 67.143\n",
      "Epoch 158: | Loss: 0.52913 | Acc: 67.143\n",
      "Epoch 159: | Loss: 0.53705 | Acc: 70.000\n",
      "Epoch 160: | Loss: 0.51532 | Acc: 71.429\n",
      "Epoch 161: | Loss: 0.52680 | Acc: 72.857\n",
      "Epoch 162: | Loss: 0.53582 | Acc: 67.143\n",
      "Epoch 163: | Loss: 0.52767 | Acc: 68.571\n",
      "Epoch 164: | Loss: 0.53309 | Acc: 71.429\n",
      "Epoch 165: | Loss: 0.53398 | Acc: 67.143\n",
      "Epoch 166: | Loss: 0.52675 | Acc: 67.143\n",
      "Epoch 167: | Loss: 0.53379 | Acc: 67.143\n",
      "Epoch 168: | Loss: 0.53236 | Acc: 70.000\n",
      "Epoch 169: | Loss: 0.53055 | Acc: 65.714\n",
      "Epoch 170: | Loss: 0.52410 | Acc: 68.571\n",
      "Epoch 171: | Loss: 0.53388 | Acc: 68.571\n",
      "Epoch 172: | Loss: 0.52237 | Acc: 68.571\n",
      "Epoch 173: | Loss: 0.53288 | Acc: 70.000\n",
      "Epoch 174: | Loss: 0.52823 | Acc: 71.429\n",
      "Epoch 175: | Loss: 0.52749 | Acc: 67.143\n",
      "Epoch 176: | Loss: 0.51406 | Acc: 72.857\n",
      "Epoch 177: | Loss: 0.52993 | Acc: 67.143\n",
      "Epoch 178: | Loss: 0.52563 | Acc: 70.000\n",
      "Epoch 179: | Loss: 0.52653 | Acc: 70.000\n",
      "Epoch 180: | Loss: 0.52564 | Acc: 70.000\n",
      "Epoch 181: | Loss: 0.52924 | Acc: 70.000\n",
      "Epoch 182: | Loss: 0.52663 | Acc: 70.000\n",
      "Epoch 183: | Loss: 0.52569 | Acc: 68.571\n",
      "Epoch 184: | Loss: 0.52369 | Acc: 68.571\n",
      "Epoch 185: | Loss: 0.52781 | Acc: 71.429\n",
      "Epoch 186: | Loss: 0.52927 | Acc: 65.714\n",
      "Epoch 187: | Loss: 0.52661 | Acc: 71.429\n",
      "Epoch 188: | Loss: 0.52920 | Acc: 65.714\n",
      "Epoch 189: | Loss: 0.51481 | Acc: 70.000\n",
      "Epoch 190: | Loss: 0.52200 | Acc: 71.429\n",
      "Epoch 191: | Loss: 0.51819 | Acc: 72.857\n",
      "Epoch 192: | Loss: 0.52975 | Acc: 71.429\n",
      "Epoch 193: | Loss: 0.52516 | Acc: 64.286\n",
      "Epoch 194: | Loss: 0.52396 | Acc: 68.571\n",
      "Epoch 195: | Loss: 0.51969 | Acc: 70.000\n",
      "Epoch 196: | Loss: 0.52560 | Acc: 71.429\n",
      "Epoch 197: | Loss: 0.51442 | Acc: 68.571\n",
      "Epoch 198: | Loss: 0.52550 | Acc: 70.000\n",
      "Epoch 199: | Loss: 0.52464 | Acc: 72.857\n",
      "Epoch 200: | Loss: 0.52509 | Acc: 68.571\n",
      "Epoch 201: | Loss: 0.51797 | Acc: 65.714\n",
      "Epoch 202: | Loss: 0.52100 | Acc: 70.000\n",
      "Epoch 203: | Loss: 0.52213 | Acc: 70.000\n",
      "Epoch 204: | Loss: 0.52233 | Acc: 70.000\n",
      "Epoch 205: | Loss: 0.52268 | Acc: 70.000\n",
      "Epoch 206: | Loss: 0.51315 | Acc: 67.143\n",
      "Epoch 207: | Loss: 0.51716 | Acc: 62.857\n",
      "Epoch 208: | Loss: 0.52274 | Acc: 70.000\n",
      "Epoch 209: | Loss: 0.52135 | Acc: 72.857\n",
      "Epoch 210: | Loss: 0.51753 | Acc: 72.857\n",
      "Epoch 211: | Loss: 0.52434 | Acc: 68.571\n",
      "Epoch 212: | Loss: 0.51829 | Acc: 68.571\n",
      "Epoch 213: | Loss: 0.51743 | Acc: 70.000\n",
      "Epoch 214: | Loss: 0.51750 | Acc: 71.429\n",
      "Epoch 215: | Loss: 0.51949 | Acc: 67.143\n",
      "Epoch 216: | Loss: 0.51752 | Acc: 74.286\n",
      "Epoch 217: | Loss: 0.50161 | Acc: 74.286\n",
      "Epoch 218: | Loss: 0.52103 | Acc: 65.714\n",
      "Epoch 219: | Loss: 0.51599 | Acc: 70.000\n",
      "Epoch 220: | Loss: 0.51745 | Acc: 68.571\n",
      "Epoch 221: | Loss: 0.50952 | Acc: 71.429\n",
      "Epoch 222: | Loss: 0.50467 | Acc: 72.857\n",
      "Epoch 223: | Loss: 0.51971 | Acc: 71.429\n",
      "Epoch 224: | Loss: 0.50521 | Acc: 72.857\n",
      "Epoch 225: | Loss: 0.51343 | Acc: 71.429\n",
      "Epoch 226: | Loss: 0.52106 | Acc: 72.857\n",
      "Epoch 227: | Loss: 0.49883 | Acc: 74.286\n",
      "Epoch 228: | Loss: 0.49649 | Acc: 71.429\n",
      "Epoch 229: | Loss: 0.52456 | Acc: 70.000\n",
      "Epoch 230: | Loss: 0.50864 | Acc: 71.429\n",
      "Epoch 231: | Loss: 0.51270 | Acc: 71.429\n",
      "Epoch 232: | Loss: 0.50403 | Acc: 71.429\n",
      "Epoch 233: | Loss: 0.51696 | Acc: 70.000\n",
      "Epoch 234: | Loss: 0.51109 | Acc: 70.000\n",
      "Epoch 235: | Loss: 0.51711 | Acc: 70.000\n",
      "Epoch 236: | Loss: 0.51123 | Acc: 71.429\n",
      "Epoch 237: | Loss: 0.51401 | Acc: 71.429\n",
      "Epoch 238: | Loss: 0.51078 | Acc: 71.429\n",
      "Epoch 239: | Loss: 0.49804 | Acc: 68.571\n",
      "Epoch 240: | Loss: 0.51176 | Acc: 70.000\n",
      "Epoch 241: | Loss: 0.51533 | Acc: 68.571\n",
      "Epoch 242: | Loss: 0.51413 | Acc: 72.857\n",
      "Epoch 243: | Loss: 0.50806 | Acc: 72.857\n",
      "Epoch 244: | Loss: 0.50711 | Acc: 70.000\n",
      "Epoch 245: | Loss: 0.51045 | Acc: 71.429\n",
      "Epoch 246: | Loss: 0.50638 | Acc: 72.857\n",
      "Epoch 247: | Loss: 0.51162 | Acc: 70.000\n",
      "Epoch 248: | Loss: 0.50481 | Acc: 70.000\n",
      "Epoch 249: | Loss: 0.50683 | Acc: 71.429\n",
      "Epoch 250: | Loss: 0.50908 | Acc: 71.429\n",
      "Epoch 251: | Loss: 0.50872 | Acc: 70.000\n",
      "Epoch 252: | Loss: 0.51348 | Acc: 74.286\n",
      "Epoch 253: | Loss: 0.50953 | Acc: 68.571\n",
      "Epoch 254: | Loss: 0.50690 | Acc: 68.571\n",
      "Epoch 255: | Loss: 0.50017 | Acc: 67.143\n",
      "Epoch 256: | Loss: 0.49770 | Acc: 72.857\n",
      "Epoch 257: | Loss: 0.50499 | Acc: 72.857\n",
      "Epoch 258: | Loss: 0.49981 | Acc: 72.857\n",
      "Epoch 259: | Loss: 0.50666 | Acc: 67.143\n",
      "Epoch 260: | Loss: 0.51132 | Acc: 71.429\n",
      "Epoch 261: | Loss: 0.49948 | Acc: 71.429\n",
      "Epoch 262: | Loss: 0.50854 | Acc: 70.000\n",
      "Epoch 263: | Loss: 0.50413 | Acc: 70.000\n",
      "Epoch 264: | Loss: 0.50413 | Acc: 70.000\n",
      "Epoch 265: | Loss: 0.49885 | Acc: 71.429\n",
      "Epoch 266: | Loss: 0.50720 | Acc: 70.000\n",
      "Epoch 267: | Loss: 0.50096 | Acc: 70.000\n",
      "Epoch 268: | Loss: 0.50716 | Acc: 71.429\n",
      "Epoch 269: | Loss: 0.48202 | Acc: 75.714\n",
      "Epoch 270: | Loss: 0.50706 | Acc: 70.000\n",
      "Epoch 271: | Loss: 0.50697 | Acc: 72.857\n",
      "Epoch 272: | Loss: 0.50319 | Acc: 70.000\n",
      "Epoch 273: | Loss: 0.50167 | Acc: 68.571\n",
      "Epoch 274: | Loss: 0.50428 | Acc: 70.000\n",
      "Epoch 275: | Loss: 0.50032 | Acc: 71.429\n",
      "Epoch 276: | Loss: 0.50027 | Acc: 74.286\n",
      "Epoch 277: | Loss: 0.50250 | Acc: 71.429\n",
      "Epoch 278: | Loss: 0.49031 | Acc: 72.857\n",
      "Epoch 279: | Loss: 0.49630 | Acc: 71.429\n",
      "Epoch 280: | Loss: 0.50778 | Acc: 70.000\n",
      "Epoch 281: | Loss: 0.50887 | Acc: 71.429\n",
      "Epoch 282: | Loss: 0.50589 | Acc: 72.857\n",
      "Epoch 283: | Loss: 0.49705 | Acc: 70.000\n",
      "Epoch 284: | Loss: 0.50309 | Acc: 70.000\n",
      "Epoch 285: | Loss: 0.48820 | Acc: 74.286\n",
      "Epoch 286: | Loss: 0.49874 | Acc: 70.000\n",
      "Epoch 287: | Loss: 0.49492 | Acc: 72.857\n",
      "Epoch 288: | Loss: 0.50030 | Acc: 71.429\n",
      "Epoch 289: | Loss: 0.48402 | Acc: 78.571\n",
      "Epoch 290: | Loss: 0.50295 | Acc: 72.857\n",
      "Epoch 291: | Loss: 0.50169 | Acc: 72.857\n",
      "Epoch 292: | Loss: 0.49826 | Acc: 68.571\n",
      "Epoch 293: | Loss: 0.49364 | Acc: 74.286\n",
      "Epoch 294: | Loss: 0.49700 | Acc: 71.429\n",
      "Epoch 295: | Loss: 0.50085 | Acc: 71.429\n",
      "Epoch 296: | Loss: 0.49958 | Acc: 74.286\n",
      "Epoch 297: | Loss: 0.49728 | Acc: 68.571\n",
      "Epoch 298: | Loss: 0.49344 | Acc: 72.857\n",
      "Epoch 299: | Loss: 0.50257 | Acc: 71.429\n",
      "Epoch 300: | Loss: 0.49131 | Acc: 72.857\n",
      "Epoch 301: | Loss: 0.49319 | Acc: 75.714\n",
      "Epoch 302: | Loss: 0.49118 | Acc: 72.857\n",
      "Epoch 303: | Loss: 0.50038 | Acc: 68.571\n",
      "Epoch 304: | Loss: 0.48706 | Acc: 74.286\n",
      "Epoch 305: | Loss: 0.49937 | Acc: 68.571\n",
      "Epoch 306: | Loss: 0.48738 | Acc: 74.286\n",
      "Epoch 307: | Loss: 0.49676 | Acc: 72.857\n",
      "Epoch 308: | Loss: 0.47673 | Acc: 75.714\n",
      "Epoch 309: | Loss: 0.49184 | Acc: 67.143\n",
      "Epoch 310: | Loss: 0.49269 | Acc: 72.857\n",
      "Epoch 311: | Loss: 0.49315 | Acc: 74.286\n",
      "Epoch 312: | Loss: 0.49711 | Acc: 71.429\n",
      "Epoch 313: | Loss: 0.48103 | Acc: 77.143\n",
      "Epoch 314: | Loss: 0.48479 | Acc: 70.000\n",
      "Epoch 315: | Loss: 0.48321 | Acc: 77.143\n",
      "Epoch 316: | Loss: 0.49085 | Acc: 72.857\n",
      "Epoch 317: | Loss: 0.49139 | Acc: 72.857\n",
      "Epoch 318: | Loss: 0.48267 | Acc: 70.000\n",
      "Epoch 319: | Loss: 0.48526 | Acc: 70.000\n",
      "Epoch 320: | Loss: 0.49248 | Acc: 74.286\n",
      "Epoch 321: | Loss: 0.49598 | Acc: 74.286\n",
      "Epoch 322: | Loss: 0.47463 | Acc: 74.286\n",
      "Epoch 323: | Loss: 0.49287 | Acc: 71.429\n",
      "Epoch 324: | Loss: 0.49153 | Acc: 71.429\n",
      "Epoch 325: | Loss: 0.48920 | Acc: 75.714\n",
      "Epoch 326: | Loss: 0.48201 | Acc: 71.429\n",
      "Epoch 327: | Loss: 0.47816 | Acc: 72.857\n",
      "Epoch 328: | Loss: 0.49150 | Acc: 74.286\n",
      "Epoch 329: | Loss: 0.49541 | Acc: 71.429\n",
      "Epoch 330: | Loss: 0.48640 | Acc: 72.857\n",
      "Epoch 331: | Loss: 0.48502 | Acc: 72.857\n",
      "Epoch 332: | Loss: 0.47744 | Acc: 72.857\n",
      "Epoch 333: | Loss: 0.49985 | Acc: 71.429\n",
      "Epoch 334: | Loss: 0.48500 | Acc: 75.714\n",
      "Epoch 335: | Loss: 0.49329 | Acc: 74.286\n",
      "Epoch 336: | Loss: 0.49757 | Acc: 72.857\n",
      "Epoch 337: | Loss: 0.48372 | Acc: 74.286\n",
      "Epoch 338: | Loss: 0.48381 | Acc: 72.857\n",
      "Epoch 339: | Loss: 0.48853 | Acc: 75.714\n",
      "Epoch 340: | Loss: 0.46829 | Acc: 80.000\n",
      "Epoch 341: | Loss: 0.48182 | Acc: 74.286\n",
      "Epoch 342: | Loss: 0.48384 | Acc: 72.857\n",
      "Epoch 343: | Loss: 0.48653 | Acc: 74.286\n",
      "Epoch 344: | Loss: 0.47963 | Acc: 74.286\n",
      "Epoch 345: | Loss: 0.48283 | Acc: 68.571\n",
      "Epoch 346: | Loss: 0.48267 | Acc: 72.857\n",
      "Epoch 347: | Loss: 0.48167 | Acc: 72.857\n",
      "Epoch 348: | Loss: 0.47612 | Acc: 77.143\n",
      "Epoch 349: | Loss: 0.48260 | Acc: 72.857\n",
      "Epoch 350: | Loss: 0.48664 | Acc: 74.286\n",
      "Epoch 351: | Loss: 0.48177 | Acc: 70.000\n",
      "Epoch 352: | Loss: 0.47771 | Acc: 72.857\n",
      "Epoch 353: | Loss: 0.47777 | Acc: 75.714\n",
      "Epoch 354: | Loss: 0.48171 | Acc: 77.143\n",
      "Epoch 355: | Loss: 0.48431 | Acc: 70.000\n",
      "Epoch 356: | Loss: 0.47953 | Acc: 74.286\n",
      "Epoch 357: | Loss: 0.47952 | Acc: 74.286\n",
      "Epoch 358: | Loss: 0.48593 | Acc: 72.857\n",
      "Epoch 359: | Loss: 0.47862 | Acc: 74.286\n",
      "Epoch 360: | Loss: 0.48175 | Acc: 72.857\n",
      "Epoch 361: | Loss: 0.47958 | Acc: 74.286\n",
      "Epoch 362: | Loss: 0.48323 | Acc: 75.714\n",
      "Epoch 363: | Loss: 0.47412 | Acc: 74.286\n",
      "Epoch 364: | Loss: 0.48139 | Acc: 75.714\n",
      "Epoch 365: | Loss: 0.48376 | Acc: 72.857\n",
      "Epoch 366: | Loss: 0.47047 | Acc: 72.857\n",
      "Epoch 367: | Loss: 0.48032 | Acc: 71.429\n",
      "Epoch 368: | Loss: 0.47754 | Acc: 77.143\n",
      "Epoch 369: | Loss: 0.48709 | Acc: 70.000\n",
      "Epoch 370: | Loss: 0.48224 | Acc: 74.286\n",
      "Epoch 371: | Loss: 0.46900 | Acc: 75.714\n",
      "Epoch 372: | Loss: 0.48025 | Acc: 72.857\n",
      "Epoch 373: | Loss: 0.48282 | Acc: 71.429\n",
      "Epoch 374: | Loss: 0.47198 | Acc: 72.857\n",
      "Epoch 375: | Loss: 0.48995 | Acc: 71.429\n",
      "Epoch 376: | Loss: 0.47895 | Acc: 74.286\n",
      "Epoch 377: | Loss: 0.47634 | Acc: 74.286\n",
      "Epoch 378: | Loss: 0.47411 | Acc: 74.286\n",
      "Epoch 379: | Loss: 0.47567 | Acc: 71.429\n",
      "Epoch 380: | Loss: 0.47256 | Acc: 72.857\n",
      "Epoch 381: | Loss: 0.47236 | Acc: 75.714\n",
      "Epoch 382: | Loss: 0.47317 | Acc: 75.714\n",
      "Epoch 383: | Loss: 0.47218 | Acc: 75.714\n",
      "Epoch 384: | Loss: 0.47344 | Acc: 74.286\n",
      "Epoch 385: | Loss: 0.47319 | Acc: 72.857\n",
      "Epoch 386: | Loss: 0.47032 | Acc: 75.714\n",
      "Epoch 387: | Loss: 0.48535 | Acc: 72.857\n",
      "Epoch 388: | Loss: 0.47525 | Acc: 74.286\n",
      "Epoch 389: | Loss: 0.46490 | Acc: 72.857\n",
      "Epoch 390: | Loss: 0.47653 | Acc: 72.857\n",
      "Epoch 391: | Loss: 0.47002 | Acc: 77.143\n",
      "Epoch 392: | Loss: 0.47603 | Acc: 72.857\n",
      "Epoch 393: | Loss: 0.46931 | Acc: 72.857\n",
      "Epoch 394: | Loss: 0.47363 | Acc: 71.429\n",
      "Epoch 395: | Loss: 0.48323 | Acc: 71.429\n",
      "Epoch 396: | Loss: 0.46945 | Acc: 77.143\n",
      "Epoch 397: | Loss: 0.47054 | Acc: 71.429\n",
      "Epoch 398: | Loss: 0.47473 | Acc: 75.714\n",
      "Epoch 399: | Loss: 0.46881 | Acc: 75.714\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch, y_batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        acc = binary_acc(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  8],\n",
       "       [ 0, 16]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.43      0.60        14\n",
      "           1       0.67      1.00      0.80        16\n",
      "\n",
      "    accuracy                           0.73        30\n",
      "   macro avg       0.83      0.71      0.70        30\n",
      "weighted avg       0.82      0.73      0.71        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
