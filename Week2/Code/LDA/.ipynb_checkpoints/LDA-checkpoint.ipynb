{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "- Graphics\n",
    "- MS Windows\n",
    "- IBM PC Hardware\n",
    "- Mac Hardware\n",
    "- Windows\n",
    "- Forsale\n",
    "- Autos\n",
    "- Motorcycles\n",
    "- Baseball\n",
    "- Hockey\n",
    "- Politics: MISC\n",
    "- Politics: Guns\n",
    "- Politics: Mideast\n",
    "- Cryptography\n",
    "- Electronics\n",
    "- Medicine \n",
    "- Space\n",
    "- Religion\n",
    "- Atheism\n",
    "- Christianity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['n.Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n.Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "      <td>18841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>\\nNot in isolated ground recepticles (usually ...</td>\n",
       "      <td>18842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>18843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n",
       "      <td>18844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>After a tip from Gary Crum (crum@fcom.cc.utah....</td>\n",
       "      <td>18845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  n.Text  index\n",
       "0      \\n\\nI am sure some bashers of Pens fans are pr...      0\n",
       "1      My brother is in the market for a high-perform...      1\n",
       "2      \\n\\n\\n\\n\\tFinally you said what you dream abou...      2\n",
       "3      \\nThink!\\n\\nIt's the SCSI card doing the DMA t...      3\n",
       "4      1)    I have an old Jasmine drive which I cann...      4\n",
       "...                                                  ...    ...\n",
       "18841  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...  18841\n",
       "18842  \\nNot in isolated ground recepticles (usually ...  18842\n",
       "18843  I just installed a DX2-66 CPU in a clone mothe...  18843\n",
       "18844  \\nWouldn't this require a hyper-sphere.  In 3-...  18844\n",
       "18845  After a tip from Gary Crum (crum@fcom.cc.utah....  18845\n",
       "\n",
       "[18846 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['n.Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ''\n",
    "for i in range(len(df)):\n",
    "    s += df['n.Text'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_strings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n.Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "      <td>18841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>\\nNot in isolated ground recepticles (usually ...</td>\n",
       "      <td>18842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>18843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n",
       "      <td>18844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>After a tip from Gary Crum (crum@fcom.cc.utah....</td>\n",
       "      <td>18845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  n.Text  index\n",
       "0      \\n\\nI am sure some bashers of Pens fans are pr...      0\n",
       "1      My brother is in the market for a high-perform...      1\n",
       "2      \\n\\n\\n\\n\\tFinally you said what you dream abou...      2\n",
       "3      \\nThink!\\n\\nIt's the SCSI card doing the DMA t...      3\n",
       "4      1)    I have an old Jasmine drive which I cann...      4\n",
       "...                                                  ...    ...\n",
       "18841  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...  18841\n",
       "18842  \\nNot in isolated ground recepticles (usually ...  18842\n",
       "18843  I just installed a DX2-66 CPU in a clone mothe...  18843\n",
       "18844  \\nWouldn't this require a hyper-sphere.  In 3-...  18844\n",
       "18845  After a tip from Gary Crum (crum@fcom.cc.utah....  18845\n",
       "\n",
       "[18846 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/akomand/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['My', 'brother', 'is', 'in', 'the', 'market', 'for', 'a', 'high-performance', 'video', 'card', 'that', 'supports\\nVESA', 'local', 'bus', 'with', '1-2MB', 'RAM.', '', 'Does', 'anyone', 'have', 'suggestions/ideas', 'on:\\n\\n', '', '-', 'Diamond', 'Stealth', 'Pro', 'Local', 'Bus\\n\\n', '', '-', 'Orchid', 'Farenheit', '1280\\n\\n', '', '-', 'ATI', 'Graphics', 'Ultra', 'Pro\\n\\n', '', '-', 'Any', 'other', 'high-performance', 'VLB', 'card\\n\\n\\nPlease', 'post', 'or', 'email.', '', 'Thank', 'you!\\n\\n', '', '-', 'Matt\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['brother', 'market', 'high', 'perform', 'video', 'card', 'support', 'vesa', 'local', 'suggest', 'idea', 'diamond', 'stealth', 'local', 'orchid', 'farenheit', 'graphic', 'ultra', 'high', 'perform', 'card', 'post', 'email', 'thank', 'matt']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = text_df[text_df['index'] == 1].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = text_df['n.Text'].fillna('').astype(str).map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docs = processed_docs.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [sure, basher, pen, fan, pretti, confus, lack,...\n",
       "1        [brother, market, high, perform, video, card, ...\n",
       "2        [final, say, dream, mediterranean, area, great...\n",
       "3        [think, scsi, card, transfer, disk, scsi, card...\n",
       "4        [jasmin, drive, understand, upsat, driver, mod...\n",
       "                               ...                        \n",
       "18841    [nyeda, cnsvax, uwec, david, neurolog, consult...\n",
       "18842    [isol, grind, recepticl, usual, unusu, color, ...\n",
       "18843    [instal, clone, motherboard, tri, mount, coole...\n",
       "18844    [wouldn, requir, hyper, sphere, space, point, ...\n",
       "18845    [gari, crum, crum, fcom, utah, phone, pontiac,...\n",
       "Name: n.Text, Length: 18846, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 actual\n",
      "1 basher\n",
      "2 beat\n",
      "3 better\n",
      "4 bowman\n",
      "5 confus\n",
      "6 coupl\n",
      "7 devil\n",
      "8 disappoint\n",
      "9 fan\n",
      "10 final\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(22, 1),\n",
       " (139, 1),\n",
       " (154, 1),\n",
       " (235, 1),\n",
       " (236, 2),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 1),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 1)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 22 (\"post\") appears 1 time.\n",
      "Word 139 (\"task\") appears 1 time.\n",
      "Word 154 (\"modern\") appears 1 time.\n",
      "Word 235 (\"anim\") appears 1 time.\n",
      "Word 236 (\"blood\") appears 2 time.\n",
      "Word 237 (\"cheer\") appears 1 time.\n",
      "Word 238 (\"comput\") appears 1 time.\n",
      "Word 239 (\"cultur\") appears 1 time.\n",
      "Word 240 (\"current\") appears 1 time.\n",
      "Word 241 (\"hard\") appears 1 time.\n",
      "Word 242 (\"kent\") appears 1 time.\n",
      "Word 243 (\"lamb\") appears 1 time.\n",
      "Word 244 (\"relat\") appears 1 time.\n",
      "Word 245 (\"sacrific\") appears 1 time.\n",
      "Word 246 (\"state\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_5 = bow_corpus[10]\n",
    "for i in range(len(bow_doc_5)):\n",
    "    print(f\"Word {bow_doc_5[i][0]} (\\\"{dictionary[bow_doc_5[i][0]]}\\\") appears {bow_doc_5[i][1]} time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06319792382897706),\n",
      " (1, 0.10246036736355967),\n",
      " (2, 0.061710242365574125),\n",
      " (3, 0.09975851946803338),\n",
      " (4, 0.08238279748384038),\n",
      " (5, 0.23536072668007965),\n",
      " (6, 0.13822244145478094),\n",
      " (7, 0.11050171009577564),\n",
      " (8, 0.07981703856801667),\n",
      " (9, 0.1399209164870263),\n",
      " (10, 0.09946920497630495),\n",
      " (11, 0.11367452628270869),\n",
      " (12, 0.30795547706619975),\n",
      " (13, 0.12937088626962795),\n",
      " (14, 0.07832611857184822),\n",
      " (15, 0.07196040332975713),\n",
      " (16, 0.09638254809927262),\n",
      " (17, 0.08144546250537524),\n",
      " (18, 0.1182095136302501),\n",
      " (19, 0.6557099554957154),\n",
      " (20, 0.1189337805247751),\n",
      " (21, 0.10770117391775481),\n",
      " (22, 0.05410409791300126),\n",
      " (23, 0.14037696057914037),\n",
      " (24, 0.07708178112664386),\n",
      " (25, 0.14083219298026672),\n",
      " (26, 0.07648309380105577),\n",
      " (27, 0.20455305632255547),\n",
      " (28, 0.13272528373655387),\n",
      " (29, 0.14376226323393773),\n",
      " (30, 0.08631374062181993),\n",
      " (31, 0.19305217187772214),\n",
      " (32, 0.07589903825126124),\n",
      " (33, 0.11427203186535144),\n",
      " (34, 0.05951642995799403),\n",
      " (35, 0.037416902892527956),\n",
      " (36, 0.08751736749844237),\n",
      " (37, 0.10236827099351915)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(22, 0.1248486548127264),\n",
       " (139, 0.26729575344838996),\n",
       " (154, 0.2406355393653195),\n",
       " (235, 0.2327284376014387),\n",
       " (236, 0.47545285845912844),\n",
       " (237, 0.24568532506601917),\n",
       " (238, 0.2207819587483869),\n",
       " (239, 0.24250515624765553),\n",
       " (240, 0.1664109385271022),\n",
       " (241, 0.1661126770309119),\n",
       " (242, 0.28316096411917724),\n",
       " (243, 0.3517542497864101),\n",
       " (244, 0.19684090823190872),\n",
       " (245, 0.2972338971797536),\n",
       " (246, 0.14365285538012476)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.018*\"drive\" + 0.012*\"chip\" + 0.011*\"scsi\" + 0.011*\"disk\" + 0.009*\"like\" + 0.008*\"know\" + 0.008*\"need\" + 0.007*\"work\" + 0.007*\"problem\" + 0.007*\"time\"\n",
      "Topic: 1 \n",
      "Words: 0.014*\"christian\" + 0.011*\"peopl\" + 0.009*\"know\" + 0.008*\"jesu\" + 0.008*\"right\" + 0.007*\"think\" + 0.007*\"believ\" + 0.007*\"say\" + 0.007*\"bibl\" + 0.006*\"word\"\n",
      "Topic: 2 \n",
      "Words: 0.020*\"window\" + 0.009*\"file\" + 0.008*\"softwar\" + 0.008*\"program\" + 0.007*\"includ\" + 0.007*\"mail\" + 0.007*\"version\" + 0.007*\"server\" + 0.006*\"avail\" + 0.006*\"graphic\"\n",
      "Topic: 3 \n",
      "Words: 0.023*\"armenian\" + 0.014*\"turkish\" + 0.009*\"turkey\" + 0.008*\"greek\" + 0.007*\"turk\" + 0.006*\"govern\" + 0.006*\"peopl\" + 0.005*\"armenia\" + 0.005*\"genocid\" + 0.005*\"know\"\n",
      "Topic: 4 \n",
      "Words: 0.010*\"planet\" + 0.010*\"earth\" + 0.007*\"myer\" + 0.007*\"moon\" + 0.007*\"orbit\" + 0.006*\"like\" + 0.005*\"solar\" + 0.005*\"atmospher\" + 0.005*\"spacecraft\" + 0.005*\"surfac\"\n",
      "Topic: 5 \n",
      "Words: 0.007*\"drug\" + 0.005*\"peopl\" + 0.005*\"state\" + 0.005*\"year\" + 0.005*\"like\" + 0.004*\"studi\" + 0.004*\"right\" + 0.004*\"case\" + 0.004*\"israel\" + 0.004*\"medic\"\n",
      "Topic: 6 \n",
      "Words: 0.028*\"file\" + 0.010*\"program\" + 0.010*\"widget\" + 0.009*\"applic\" + 0.007*\"entri\" + 0.007*\"output\" + 0.007*\"line\" + 0.007*\"error\" + 0.006*\"window\" + 0.006*\"function\"\n",
      "Topic: 7 \n",
      "Words: 0.015*\"know\" + 0.013*\"think\" + 0.011*\"go\" + 0.011*\"say\" + 0.011*\"time\" + 0.010*\"peopl\" + 0.009*\"like\" + 0.008*\"work\" + 0.007*\"thing\" + 0.007*\"come\"\n",
      "Topic: 8 \n",
      "Words: 0.016*\"encrypt\" + 0.010*\"secur\" + 0.009*\"govern\" + 0.008*\"key\" + 0.008*\"peopl\" + 0.007*\"chip\" + 0.007*\"know\" + 0.006*\"clipper\" + 0.006*\"algorithm\" + 0.005*\"escrow\"\n",
      "Topic: 9 \n",
      "Words: 0.008*\"space\" + 0.007*\"control\" + 0.006*\"power\" + 0.006*\"like\" + 0.005*\"work\" + 0.005*\"think\" + 0.005*\"know\" + 0.005*\"good\" + 0.004*\"design\" + 0.004*\"mail\"\n",
      "Topic: 10 \n",
      "Words: 0.008*\"presid\" + 0.008*\"state\" + 0.008*\"univers\" + 0.006*\"say\" + 0.006*\"nation\" + 0.005*\"inform\" + 0.005*\"peopl\" + 0.005*\"right\" + 0.005*\"govern\" + 0.004*\"privaci\"\n",
      "Topic: 11 \n",
      "Words: 0.010*\"think\" + 0.010*\"peopl\" + 0.008*\"believ\" + 0.008*\"like\" + 0.007*\"know\" + 0.006*\"say\" + 0.006*\"post\" + 0.006*\"point\" + 0.005*\"question\" + 0.005*\"mean\"\n",
      "Topic: 12 \n",
      "Words: 0.016*\"year\" + 0.014*\"game\" + 0.011*\"team\" + 0.009*\"think\" + 0.008*\"good\" + 0.008*\"like\" + 0.007*\"time\" + 0.007*\"player\" + 0.006*\"play\" + 0.006*\"know\"\n",
      "Topic: 13 \n",
      "Words: 0.014*\"post\" + 0.010*\"like\" + 0.008*\"list\" + 0.008*\"period\" + 0.007*\"mail\" + 0.006*\"play\" + 0.006*\"send\" + 0.006*\"read\" + 0.005*\"greek\" + 0.005*\"goal\"\n",
      "Topic: 14 \n",
      "Words: 0.009*\"wire\" + 0.009*\"armenian\" + 0.005*\"like\" + 0.005*\"say\" + 0.005*\"come\" + 0.005*\"grind\" + 0.005*\"peopl\" + 0.004*\"drive\" + 0.004*\"go\" + 0.004*\"time\"\n",
      "Topic: 15 \n",
      "Words: 0.013*\"space\" + 0.010*\"launch\" + 0.010*\"nasa\" + 0.009*\"orbit\" + 0.007*\"like\" + 0.007*\"shuttl\" + 0.006*\"time\" + 0.006*\"program\" + 0.006*\"peopl\" + 0.006*\"mission\"\n",
      "Topic: 16 \n",
      "Words: 0.026*\"book\" + 0.011*\"appear\" + 0.007*\"know\" + 0.007*\"comic\" + 0.006*\"star\" + 0.006*\"cover\" + 0.005*\"black\" + 0.005*\"write\" + 0.005*\"sell\" + 0.005*\"work\"\n",
      "Topic: 17 \n",
      "Words: 0.026*\"card\" + 0.015*\"monitor\" + 0.009*\"video\" + 0.009*\"know\" + 0.008*\"thank\" + 0.008*\"work\" + 0.007*\"drive\" + 0.006*\"keyboard\" + 0.006*\"appl\" + 0.006*\"problem\"\n",
      "Topic: 18 \n",
      "Words: 0.030*\"imag\" + 0.013*\"file\" + 0.010*\"format\" + 0.009*\"jpeg\" + 0.009*\"color\" + 0.009*\"program\" + 0.009*\"data\" + 0.008*\"space\" + 0.006*\"avail\" + 0.006*\"graphic\"\n",
      "Topic: 19 \n",
      "Words: 0.022*\"homosexu\" + 0.014*\"window\" + 0.009*\"file\" + 0.008*\"know\" + 0.008*\"work\" + 0.007*\"mous\" + 0.007*\"kinsey\" + 0.006*\"time\" + 0.006*\"think\" + 0.006*\"driver\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.005*\"encrypt\" + 0.003*\"chip\" + 0.003*\"window\" + 0.003*\"key\" + 0.003*\"escrow\" + 0.003*\"know\" + 0.003*\"need\" + 0.003*\"file\" + 0.003*\"thank\" + 0.003*\"think\"\n",
      "Topic: 1 Word: 0.005*\"game\" + 0.005*\"catcher\" + 0.003*\"time\" + 0.003*\"play\" + 0.003*\"stat\" + 0.003*\"know\" + 0.003*\"john\" + 0.003*\"think\" + 0.003*\"pitcher\" + 0.003*\"good\"\n",
      "Topic: 2 Word: 0.006*\"thank\" + 0.005*\"file\" + 0.005*\"mail\" + 0.004*\"window\" + 0.004*\"look\" + 0.004*\"know\" + 0.003*\"email\" + 0.003*\"printer\" + 0.003*\"post\" + 0.003*\"repli\"\n",
      "Topic: 3 Word: 0.007*\"game\" + 0.005*\"think\" + 0.004*\"team\" + 0.004*\"year\" + 0.004*\"cap\" + 0.004*\"peopl\" + 0.003*\"pen\" + 0.003*\"isl\" + 0.003*\"wing\" + 0.003*\"know\"\n",
      "Topic: 4 Word: 0.008*\"armenian\" + 0.004*\"turkish\" + 0.003*\"turkey\" + 0.003*\"turk\" + 0.003*\"like\" + 0.003*\"card\" + 0.003*\"greek\" + 0.002*\"know\" + 0.002*\"muslim\" + 0.002*\"good\"\n",
      "Topic: 5 Word: 0.005*\"batf\" + 0.004*\"warrant\" + 0.004*\"govern\" + 0.003*\"koresh\" + 0.003*\"time\" + 0.003*\"say\" + 0.003*\"like\" + 0.003*\"peopl\" + 0.002*\"right\" + 0.002*\"think\"\n",
      "Topic: 6 Word: 0.003*\"error\" + 0.003*\"like\" + 0.003*\"know\" + 0.003*\"test\" + 0.002*\"think\" + 0.002*\"program\" + 0.002*\"game\" + 0.002*\"point\" + 0.002*\"work\" + 0.002*\"post\"\n",
      "Topic: 7 Word: 0.007*\"card\" + 0.007*\"window\" + 0.004*\"driver\" + 0.004*\"monitor\" + 0.004*\"thank\" + 0.004*\"video\" + 0.004*\"drive\" + 0.004*\"program\" + 0.004*\"color\" + 0.004*\"work\"\n",
      "Topic: 8 Word: 0.004*\"peopl\" + 0.004*\"drug\" + 0.003*\"kill\" + 0.003*\"know\" + 0.002*\"right\" + 0.002*\"year\" + 0.002*\"like\" + 0.002*\"countri\" + 0.002*\"gun\" + 0.002*\"think\"\n",
      "Topic: 9 Word: 0.006*\"church\" + 0.003*\"vers\" + 0.003*\"satan\" + 0.003*\"lord\" + 0.003*\"know\" + 0.003*\"scriptur\" + 0.003*\"interpret\" + 0.003*\"bibl\" + 0.003*\"like\" + 0.003*\"doug\"\n",
      "Topic: 10 Word: 0.005*\"pitt\" + 0.005*\"skeptic\" + 0.005*\"intellect\" + 0.005*\"cadr\" + 0.005*\"chastiti\" + 0.005*\"surrend\" + 0.005*\"gordon\" + 0.005*\"shame\" + 0.004*\"bank\" + 0.004*\"soon\"\n",
      "Topic: 11 Word: 0.005*\"modem\" + 0.004*\"look\" + 0.004*\"font\" + 0.004*\"window\" + 0.004*\"want\" + 0.004*\"thank\" + 0.004*\"softwar\" + 0.003*\"work\" + 0.003*\"file\" + 0.003*\"post\"\n",
      "Topic: 12 Word: 0.004*\"bike\" + 0.003*\"compuserv\" + 0.003*\"thank\" + 0.003*\"like\" + 0.003*\"copyright\" + 0.003*\"address\" + 0.003*\"cramer\" + 0.003*\"know\" + 0.003*\"year\" + 0.002*\"mail\"\n",
      "Topic: 13 Word: 0.005*\"game\" + 0.004*\"team\" + 0.003*\"bike\" + 0.003*\"think\" + 0.003*\"hockey\" + 0.003*\"say\" + 0.003*\"know\" + 0.003*\"like\" + 0.003*\"peopl\" + 0.003*\"go\"\n",
      "Topic: 14 Word: 0.005*\"christian\" + 0.005*\"peopl\" + 0.005*\"homosexu\" + 0.004*\"think\" + 0.003*\"jew\" + 0.003*\"know\" + 0.003*\"believ\" + 0.003*\"mean\" + 0.003*\"say\" + 0.003*\"religion\"\n",
      "Topic: 15 Word: 0.006*\"game\" + 0.004*\"captain\" + 0.004*\"test\" + 0.004*\"team\" + 0.003*\"gant\" + 0.003*\"think\" + 0.003*\"pitcher\" + 0.003*\"adress\" + 0.003*\"fan\" + 0.003*\"april\"\n",
      "Topic: 16 Word: 0.007*\"file\" + 0.004*\"thank\" + 0.004*\"window\" + 0.004*\"know\" + 0.004*\"program\" + 0.003*\"imag\" + 0.003*\"think\" + 0.003*\"format\" + 0.003*\"need\" + 0.003*\"code\"\n",
      "Topic: 17 Word: 0.010*\"drive\" + 0.007*\"scsi\" + 0.006*\"thank\" + 0.005*\"window\" + 0.005*\"disk\" + 0.004*\"control\" + 0.004*\"work\" + 0.004*\"simm\" + 0.004*\"know\" + 0.004*\"card\"\n",
      "Topic: 18 Word: 0.004*\"drive\" + 0.003*\"time\" + 0.003*\"like\" + 0.003*\"insur\" + 0.003*\"disk\" + 0.003*\"window\" + 0.002*\"peopl\" + 0.002*\"problem\" + 0.002*\"know\" + 0.002*\"price\"\n",
      "Topic: 19 Word: 0.003*\"think\" + 0.003*\"clinton\" + 0.003*\"peopl\" + 0.003*\"govern\" + 0.003*\"post\" + 0.003*\"israel\" + 0.002*\"like\" + 0.002*\"year\" + 0.002*\"time\" + 0.002*\"know\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
